---
title: "Project 1: Functions for Census Bureau Public Use Microdata Sample Processing"
author: "Cass Crews"
format: html
editor: visual
---

# Introduction

The Public Use Microdata Samples (PUMS) from the Census Bureau provide deep insights into the socio-demographic characteristics of US residents. With a few caveats, the samples contain the individual responses from each American Community Survey, giving researchers direct access to the lived experience for one percent of US residents each year. 

While the data are incredibly valuable to understanding American life and its evolution, they are not particularly easy to work with. In its raw form, each annual dataset is very large and difficult to interpret. The goal of this page is to document an automated process for obtaining and processing subsets of the data, unlocking the power of the data for researchers. We begin by describing the process of obtaining the data via the PUMS Census application programming interface (API) before transitioning to the creation of a function that reads in and cleans user-specified subsets of an annual PUMS dataset. Finally, we detail the creation of a wrapper for this function that combines multiple years of cleaned data. As we proceed down the page, examples will be provided to highlight the outcomes of our work. 

# Loading in key packages

Before we begin, we need to load required packages. I have printed them below, but I have suppressed all messages and warnings to preserve the tidyness of the page. After all, we are utilizing the `tidyverse`! 

```{r}
#| message: FALSE
#| warning: FALSE
#Loading required packages
library(tidyverse)
library(httr)
library(jsonlite)
library(stringr)
library(lubridate)
library(hms)
```

# Understanding the Census API

To obtain PUMS data via the API, we can take the standard approach of using `httr::GET()` and determining the correct URL structure to request the data we want. 

As an example, we may want to obtain educational attainment for survey respondents living in Alabama in 2022. Let's break down the corresponding URL:

> `https://api.census.gov/data/2022/acs/acs1/pums?get=PWGTP,SCHL&for=state:01`

The portion of the URL up to the `?` (`https://api.census.gov/data/2022/acs/acs1/pums`) indicates that we want data from the ACS 2022 1-year PUMS file. `get=PWGTP,SCHL,AGEP` indicates we want the person-level weights (`PWGTP`), which we will use when generating tabulations to ensure our tabulations are representative of the Alabama population, as well as individuals' educational attainment (`SCHL`) and age (`AGEP`). `for=state:01` indicates we only want data for Alabama. 

Let's use `GET()` to read in our data and see what we get. 

```{r}
#Making a standard API call
example_data<-
  GET("https://api.census.gov/data/2022/acs/acs1/pums?get=PWGTP,SCHL,AGEP&for=state:01")

#Printing the high-level structure of the response
str(example_data,max.level=1)
```

There is a lot of information returned by the Census API, but what we actually want is the raw, currently uninterpretable information in `example_data$content`. To extract these data in a usable form, we can combine the `rawToChar()` function, which will convert the raw data to human-interpretable data, with the `fromJSON()` function from the `jsonlite` package, which will convert our data to a matrix. As a final step, we can coerce our data to a tibble. This will result in cleaner printing. 

```{r}
#Converting the data/content to a matrix
example_mat<-fromJSON(rawToChar(example_data$content))

#Assigning column names, which are in the first row
colnames(example_mat)<-example_mat[1,]
example_mat<-example_mat[-1,]

#Converting to tibble and converting PWGTP to a numeric variable
example_tbl<-as_tibble(example_mat) |>
  mutate(PWGTP=as.numeric(PWGTP))

#Printing the tibble
example_tbl
```

This is still not particularly useful as educational attainment (`SCHL`) is a factor variable; the values are codes with explicit meaning. Similarly, the values of `state` are all `01` rather than `Alabama`. 

Additionally, it would be nice to not need to write out all the code to complete all the post-`GET` processing each time we make an API call. To deal with this issue, let's write a helper function that completes the data processing for us. 

```{r}
#Creating an API response cleaner "helper" function
content_cleaner<-function(response) {
  #Capturing the data in a matrix
  mat<-fromJSON(rawToChar(response$content))
  
  #Assigning column names (in the first row)
  colnames(mat)<-mat[1,]
  mat<-mat[-1,]
  
  #Converting to a tibble and converting PWGTP to numeric
  tbl<-as_tibble(mat) |>
    mutate(PWGTP=as.numeric(PWGTP))

  #Returning the tibble
  return(tbl)
}
```

That should speed up the process. Let's test the function on our original API response to ensure it works.

```{r}
#Applying the helper function to our initial API response
test_tbl<-content_cleaner(example_data)

#Printing the resulting tibble
test_tbl
```

It looks like our function worked!

# A Function to Query Census API and Clean Data

We still face the issue that our data aren't as informative as they could be. Also, it would be nice to be able to change our request and still have the resulting tibble be an informative and useful dataset. 

To solve these current limitations, let's build a comprehensive query function that allows the user to do the following:

- Select the survey year
- Select from a set of useful numeric variables, and return these data in the appropriate type
- Select from a set of useful categorical variables and return these data as factor variables with appropriate levels
- Specify a geography level to report
- Optionally subset the data to specific geographies within the specified geography level

Let's design the function so it returns a tibble that is informative and ready for analysis.

The resultant `census_get()` function is documented in the code chunk below. Note that there are five arguments consistent with our goals for the function:

- `year`: The survey year we want to obtain data from, which cannot be `NULL` or multiple years
  - Valid Inputs are any year from 2010 to 2023, excluding 2020; the default is 2022
- `numvar`: A vector indicating the numeric variables we want to return, which cannot be `NULL`. Available variables:
  - `AGEP`: The age of the individual in years; this variable is selected by default
  - `GASP`: Monthly gas cost in dollars
  - `GRPIP`: Gross rent as a percentage of household income over the past 12 months
  - `JWAP`: Time of arrival at work; survey respondents select from 5-minute windows, which the function converts to a single time value using the midpoint of the window
  - `JWDP`: Time of departure for work; survey respondents select from 10-minute windows, which the function converts to a single time value using the midpoint of the window
  - `JWMNP`: Travel time to work in minutes
- `catvar`: A vector indicating the categorical variables we want to return. Available variables:
  - `FER`: Indicates whether the individual gave birth to a child within the past 12 months 
  - `HHL`: Language spoken at home
  - `HISPEED`: Indicates whether the individual has broadband internet service; `HISPEED` can only be requested for 2016 or later
  - `JWTRNS`: Means of transportation to work; `JWTRNS` can only be requested for 2019 or later 
  - `SCH`: Indicates whether the individual is enrolled in school
  - `SCHL`: Highest level of educational attainment
  - `SEX`: The sex of the individual; this variable is selected by default
- `geo_level`: The geography level to return. Options:
  - `all`: No geography level returned
  - `region`: Census regions; region names available [here](https://api.census.gov/data/2022/acs/acs1/pums/variables/REGION.json)
  - `division`: Census divisions; division names available [here](https://api.census.gov/data/2022/acs/acs1/pums/variables/DIVISION.json)
  - `state`: US states plus DC, listed [here](https://api.census.gov/data/2022/acs/acs1/pums/variables/ST.json); this level is the default
- `geo_select`: An optional vector of specific geographies of level `geo_level` to subset the returned data to. Use the links above to confirm geography spelling and formatting. By default the returned data are subset to individuals in Arizona. 

In addition to the optional variables, `PWGTP`, the number of individuals each observation "represents," is always included in the function output. This is because we need `PWGTP` to generate population-representative tabulations from the data. 

Readers are encouraged to review the `get_census()` function code for full details on function behavior. We will summarize the function steps below the code chunk. 

```{r}
#Constructing our main data requesting and cleaning function
census_get<-function(year=2022,numvar="AGEP",catvar="SEX",geo_level="state",
                     geo_select="Arizona") {

  ##############################################################################  
  #CHECKING USER INPUTS
  ##############################################################################
  
  #Confirming year input
  if (!(year %in% c(2010:2019,2021:2023))) 
    stop("year must be between 2010 and 2023, and must not be 2020") 
  #Confirming JWTRNS not requested prior to 2019
  if (year< 2019 & ("JWTRNS" %in% catvar)) 
    stop("JWTRNS unavailable prior to 2019") 
  #Confirming HISPEED not requested prior to 2016
  if (year< 2016 & ("HISPEED" %in% catvar)) 
    stop("HISPEED unavailable prior to 2016") 
  #Confirming numvar not set to NULL
  if (is.null(numvar)) 
    stop("At least one numeric variable must be selected") 
  #Confirming all numvar inputs are valid
  if(length(setdiff(numvar,c("AGEP","GASP","GRPIP","JWMNP","JWAP","JWDP")))> 0) 
    stop("At least one numeric variable not allowed") 
  #Confirming catvar not set to NULL
  if (is.null(catvar)) 
    stop("At least one categorical variable must be selected") 
  #Confirming all catvar inputs are valid
  if(length(setdiff(catvar,c("FER", "HHL", "HISPEED", "JWTRNS", "SCH",
                             "SCHL", "SEX")))> 0) 
    stop("At least one categorical variable not allowed") 
  #Confirming only one geo level specified
  if (length(geo_level)> 1) 
    stop("No more than one geographic level allowed") 
  #Confirming geo_level not set to NULL
  if (is.null(geo_level)) 
    stop("Must specify a geographic level") 
  #Confirming geo_level input is valid
  if (length(intersect(geo_level,c("all","region","division","state")))==0) 
    stop("Invalid geography level") 

  
  ##############################################################################
  #BUILDING THE URL AND MAKING API CALL
  ##############################################################################

  #CONVERTING geo_level TO CODES FOR GET URL SPECIFICATION######################  
  
  #Handling state case
  if (geo_level=="state") {
    #Reading in state variable metadata
    ##Using 2022 as metadata not provided in all years
    metadata_geo<-
      read_json(paste0("https://api.census.gov/data/2022/acs/acs1/pums/variables/ST.json"))
    
    #Capturing state fips codes and labels
    geo_dict<-data.frame(state=unlist(names(metadata_geo$values$item)),
                          labels=unlist(metadata_geo$values$item,use.names=FALSE))
    
    #Removing leading zeros to protect against code format changes across years  
    geo_dict$state<-as.character(as.numeric(geo_dict$state))
    
    #Stripping labels down to only state names 
    #(originally include postal abbreviations)  
    geo_dict$labels<-word(geo_dict$labels,1,sep="/")
    
    #When state subset specified, capturing corresponding FIPS codes for GET URL
    if (!is.null(geo_select)) {
      if (length(intersect(geo_dict$labels, geo_select))==0) 
        stop("Invalid state specified") #Confirming supplied states are valid
      
      #Capturing FIPS codes
      selected_geos<-geo_dict[(geo_dict$labels %in% geo_select),]$state
    }
    
  #Handling region case
  } else if (geo_level=="region") {
    #Reading in region variable metadata    
    metadata_geo<-read_json(paste0("https://api.census.gov/data/",year,
                                   "/acs/acs1/pums/variables/REGION.json"))
    
    #Capturing the region codes and labels
    geo_dict<-data.frame(region=unlist(names(metadata_geo$values$item)),
                          labels=unlist(metadata_geo$values$item,use.names=FALSE))    

    #When region subset specified, capturing corresponding codes for GET URL    
    if (!is.null(geo_select)) {
      if (length(intersect(geo_dict$labels, geo_select))==0) 
        stop("Invalid region specified") #Confirming supplied regions are valid

      #Capturing region codes      
      selected_geos<-geo_dict[(geo_dict$labels %in% geo_select),]$region
    } 
    
  #Handling division case
  } else if (geo_level=="division") {
    #Reading in division variable metadata
    metadata_geo<-read_json(paste0("https://api.census.gov/data/",year,
                                   "/acs/acs1/pums/variables/DIVISION.json"))
    
    #Capturing the division codes and labels
    geo_dict<-data.frame(division=unlist(names(metadata_geo$values$item)),
                          labels=unlist(metadata_geo$values$item,use.names=FALSE))
    
    #Stripping labels down to only division names 
    #(Also indicates corresponding region for 2021:2023)  
    geo_dict$labels<-word(geo_dict$labels,1,sep=fixed(" ("))       

    #When division subset specified, capturing corresponding codes for GET URL
    if (!is.null(geo_select)) {
      if (length(intersect(geo_dict$labels, geo_select))==0) 
        stop("Invalid division specified") #Confirming supplied divisions are valid
      
      #Capturing division codes
      selected_geos<-geo_dict[(geo_dict$labels %in% geo_select),]$division
    }    
  }
  
  
  #CONSTRUCTING THE URL AND MAKING THE CALL#####################################
  
  #Building the URL when all is the specified geography
  if (geo_level=="all") { 
    URL<-paste0("https://api.census.gov/data/",year,"/acs/acs1/pums?get=PWGTP,",
                paste(numvar,collapse=","),",",paste(catvar,collapse=","))
  #Building URL when another geography is specified and no specific regions 
    #are selected
  } else if (is.null(geo_select)) { 
    URL<-paste0("https://api.census.gov/data/",year,"/acs/acs1/pums?get=PWGTP,",
                paste(numvar,collapse=","),",",paste(catvar,collapse=","),"&for=",
                geo_level,":*")
  #Building URL when a subset of regions is specified
  } else { 
    URL<-paste0("https://api.census.gov/data/",year,"/acs/acs1/pums?get=PWGTP,",
                paste(numvar,collapse=","),",",paste(catvar,collapse=","),"&for=",
                geo_level,":",paste(selected_geos,collapse=","))
  }
  
  #Making API Call
  response<-GET(URL)
  

  
  ##############################################################################
  #DATA CLEANING
  ##############################################################################  
  
  #APPLYING content_cleaner HELPER FUNCTION#####################################
  
  #Applying content_cleaner helper function
  clean_response<-content_cleaner(response)
  
  
  #CONVERTING NUMERIC VARIABLES TO NUMERIC TYPE#################################
  
  #Specifying potential truly numeric variable selections
  num_options<-c("AGEP","GASP","GRPIP","JWMNP")
 
  #Converting numeric variables to numeric type
  clean_response<- clean_response |>
    mutate(across(any_of(num_options),as.numeric)) 

  
  #TIME VARIABLE PROCESSING#####################################################
    
  #Specifying potential time variable selections
  time_options<-c("JWAP","JWDP")
  
  #Identifying time variables specified by user
  time_vars<-intersect(time_options,numvar)
  
  #Looping across selected time variables to extract midpoint time values
  #for time ranges and incorporating into dataset
  for (t in time_vars) {
    #Reading in variable metadata
    metadata_time<-read_json(paste0("https://api.census.gov/data/",year,
                                    "/acs/acs1/pums/variables/",t,".json"))
    
    #Capturing variable codes and labels
    value_dict<-data.frame(levels=unlist(names(metadata_time$values$item)),
                           labels=unlist(metadata_time$values$item,use.names=FALSE))
    
    #Capturing time range endpoints
    value_dict[,3:4]<-str_split_fixed(value_dict[[2]]," to ",2)
    
    #Converting endpoints to true time data
    value_dict[3]<-parse_hm(value_dict[[3]])
    value_dict[4]<-parse_hm(value_dict[[4]])
    
    #Assigning endpoint names
    names(value_dict)[3:4]<-c("time_1","time_2")
    
    #Calculating midpoint between time endpoints and producing clean time 
    #value dictionary
    value_dict<- value_dict |>
      mutate(secs_1=as.numeric(time_1),
             secs_2=as.numeric(time_2),
             avg_time=as_hms((secs_1+secs_2)/2)) |>
      select(levels,avg_time) |>
      rename(!!t := levels)
    
    #Joining time midpoints to the dataset via the value codes and replacing codes
    #with midpoints
    clean_response<- clean_response |>
      left_join(value_dict,by=t) |>
      mutate(!!t := avg_time) |>
      select(-avg_time)
  }
  
  
  #CATEGORICAL VARIABLE PROCESSING##############################################
  
  #Looping through categorical variables and converting to factors with appropriate
  #labels
  for (c in catvar) {
    #Reading in categorical variable metadata
    metadata_cat<-read_json(paste0("https://api.census.gov/data/",year,
                                   "/acs/acs1/pums/variables/",c,".json"))
    
    #Using metadata (codes and labels) to convert categorical variables to factors
    clean_response[c]<-factor(clean_response[[c]],
                              levels=unlist(names(metadata_cat$values$item)),
                              labels=unlist(metadata_cat$values$item,use.names=FALSE))
  }
  
  
  #GEOGRAPHIC VARIABLE PROCESSING###############################################
  
  #Stripping leading zeroes from state FIPS codes to match codes with labels 
  #captured above in geo_dict
  if (geo_level=="state") 
    clean_response$state<-as.character(as.numeric(clean_response$state))
  
  #Replacing geography codes with geography names
  if (geo_level!="all") {
    clean_response<- clean_response |>
      left_join(geo_dict,by=geo_level) |>
      mutate(!!geo_level := labels) |>
      select(-labels)    
  }
  

  #MISSING VALUE HANDLING AND RETURN SPECIFICATION##############################
  
  #Changing truly missing values to missing
  #The second mutate Handles variations in NA coding across years
  clean_response<- clean_response |>
    mutate(across(any_of(c("GRPIP","JWMNP")),~na_if(.x,0))) |>
    mutate(across(any_of("GASP"),~if_else(.x<=3,NA_real_,.x))) 
  
  #Indicating we want to return the processed dataset
  return(clean_response)
}
```

The function achieves our goals via the following general steps. First, the user inputs are thoroughly evaluated to ensure no required inputs are `NULL` and all specifications for `year`, `numvar`, `catvar`, `geo_level`, and `geo_select` are valid options. When mistakes are made, informative error messages are given so the user understands their mistake. 

Second, the appropriate URL is constructed and the API call is made. This involves converting any `geo_select` inputs to the numeric codes the API can interpret, as well as incorporating the correct year and variable names into the URL. 

Third, the data are cleaned and returned. Data cleaning has the following sub-steps:

1. Our `content_cleaner()` helper function is applied to the data
2. The non-time numeric variables are converted to numeric type
3. Time variables are converted to time type using the midpoint of the time ranges
4. Categorical variables are converted to factors with appropriate labels 
5. Geography names replace codes for any geography variables
6. Missing value codes in numeric variables are changed to missing (`NA`)

Now that we have the function and generally know how it works, let's test it. To start, let's again request educational attainment and age for individuals from Alabama in the 2022 survey. 

```{r}
#Testing the function for the first time
first_tbl<-census_get(year=2022,catvar=c("SCHL"),numvar=c("AGEP"),
                          geo_level="state",geo_select="Alabama")

#Printing the results
first_tbl
```

This is much better! Age is now numeric, educational attainment is easily interpreted, and we do not need to look up Alabama's FIPS code to ensure we have the correct state. 

Let's keep testing the function by confirming our default inputs evaluate correctly. 

```{r}
#Testing the function with the default inputs
second_tbl<-census_get()

#Printing the resulting tibble
second_tbl
```

This is exactly what we expected, so it looks like we correctly specified the defaults. 

Now let's see how one of the time variables displays by adding `JWAP` to the request. When we print the resulting tibble, let's subset the data to only the individuals who provided a time they arrive at work. 

```{r}
#Testing the function when a time variable is included
third_tbl<-census_get(numvar=c("AGEP","JWAP"))

#Printing the tibble after subsetting to observations with non-missing time values
third_tbl |> 
  filter(!is.na(JWAP))
```

Again, this looks great. Not only do we have cleanly displayed times, but we have data we can analyze to understand the distribution of arrival times. 

As a final set of checks, let's intentionally make mistakes in our inputs to see how our error messages present. First, let's see what happens when we request a numeric variable that is not available. 

```{r}
#| error: true
#Demonstrating what happens when we misspecify a numeric variable
non_created_tbl1<-census_get(numvar="DOG")
```

Excellent! Not only do we get an error, but we know the error is because one of the specified numeric variables is not allowed. 

Now let's see what happens when we request data for a state that isn't actually a state.

```{r}
#| error: true
#Demonstrating what happens when we misspecify the geography to subset to
non_created_tbl2<-census_get(geo_select="Cat")
```

Now we know there is no US state named Cat. Good to know. 


# A Function to Combine Multiple Survey Years

We have created a very helpful function. However, it is often the case that our analysis requires multiple years of data, such as when we want to analyze changes in age distributions across time. Let's build a function that repeatedly deploys our `census_get` function for each year of data we request. 

The arguments and defaults for this `census_get_multiyear` function will be identical to those of the `census_get` function, except that `year` is now `years`; `years` accepts a vector of valid year values, and its default is `2021:2022`. To indicate the year an observation corresponds to, the function will include a `year` variable in the output tibble. 

```{r}
#Creating the census_get wrapper function
census_get_multiyear<-function(years=2021:2022,numvar="AGEP",catvar="SEX",
                               geo_level="state",geo_select="Arizona") {
  
  #Checking year inputs to avoid unnecessary computation if one input invalid
  if (length(setdiff(years,c(2010:2019,2021:2023)))> 0) 
    stop("At least one input year not allowed")
  
  #Looping through years to run census_get function and append each year of data 
  counter<-1
  for (y in years) {
    #Running census_get for first year and creating initial dataset
    if (counter==1) {
      data<-census_get(year=y,catvar=catvar,numvar=numvar,geo_level=geo_level,
                       geo_select=geo_select)
      
      #Capturing year of data
      data$year<-y
      
    #Running census_get for subsequent years and appending to initial dataset
    } else {
      int_data<-census_get(year=y,catvar=catvar,numvar=numvar,geo_level=geo_level,
                           geo_select=geo_select)
      
      #Capturing year of data
      int_data$year<-y
      
      #Appending to the existing dataset
      data<-bind_rows(data,int_data)
    }
    counter<-counter+1 #Updating counter for if/else 
  }
  
  #Indicating we want the full appended dataset returned
  return(data)
}
```

We can test the function to ensure it works as intends. First, let's request the recent birth indicator for individuals living in the Mountain census division in 2016 and 2023. We will produce two different filtered prints to ensure we obtain both years of data; we filter to each year and keep only females. We might request these data if we wanted to analyze changes in birth rates. 

```{r}
#Testing the wrapper for the first time
first_multiyr_tbl<-census_get_multiyear(years=c(2016,2023),catvar=c("FER","SEX"),
                                        geo_level="division",geo_select="Mountain")

#Printing the results when year is 2016 and sex is female
first_multiyr_tbl |> 
  filter(year==2016,SEX=="Female")

#Printing the results when year is 2016 and sex is female
first_multiyr_tbl |>
  filter(year==2023,SEX=="Female")
```

The new multi-year function works as intended. Additionally, by comparing age and the recent birth indicator, we see initial evidence that our factor labeling works correctly; note that females with ages below 15 or above 50 are assigned an `N/A`. 

As a final check, let's see what happens when we request a year outside the available range. 

```{r}
#| error: true
#Testing the function when we include an invalid year
non_created_multiyr_tbl<-census_get_multiyear(years=c(2016,2024),
                                          catvar=c("FER","SEX"),
                                          geo_level="division",
                                          geo_select="Mountain")
```

This worked exactly as we wanted: the function returned an error before wasting time submitting inputs to `census_get`. 

# Conclusion

This document explores the use of the Public Use Microdata Sample Census API to obtain individual responses to the American Community Survey. As these data require substantial work to get them into an analysis-ready form, we have created two key functions to automate the data pull and cleaning process. The first function is `census_get()`, which handles the data pull and cleaning process for individual survey years. The second function is `census_get_multiyear()`, which repeatedly deploys `census_get` to obtain multiple years of data in a single tibble. I hope you have found this overview of the data and functions informative, and that you find the functions useful in your future work! 